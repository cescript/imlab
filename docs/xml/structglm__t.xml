<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.5">
  <compounddef id="structglm__t" kind="struct" prot="public">
    <compoundname>glm_t</compoundname>
    <includes refid="mlcore_8h" local="no">mlcore.h</includes>
    <briefdescription>
<para><ref refid="structglm__t" kindref="compound">glm_t</ref> struct keeps the given GLM parameters in a single variable and computes the necessary parameters at the construction step. </para>    </briefdescription>
    <detaileddescription>
<para>Generalized Linear Models are machine learning techniques that approximates the output labels <formula id="19">$y$</formula> as the following linear form. <formula id="20">\[y = w \times x + b\]</formula> Here <formula id="21">$w$</formula> is the linear coefficient and <formula id="22">$b$</formula> is the bias. The problem of the GLM are to find the best coefficient vector and bias value depending on the given restrictions on these vectors. In the most generalized way the cost function can be defined as the sum of the classification and regularization loss <formula id="23">\[C(w,b) := \frac{1}{n} \sum_i { L( y_i, a_i = w_i \times x_i+b) } + R( w, \lambda )\]</formula></para><para>Using the above cost function one can create different sets of problems/solutions by simply changing the lost function <formula id="24">$L$</formula> and regularization function <formula id="16">$R$</formula>. Here a list of supported lost and regularization functions are given.</para><para><table rows="12" cols="3"><row>
<entry thead="yes"><para>algorithm</para></entry><entry thead="yes"><para>lost function <formula id="25">$(L)$</formula></para></entry><entry thead="yes"><para>regularization <formula id="26">$(R)$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Least-squares without regularization</para></entry><entry thead="no"><para><formula id="27">$L = (a - y)^2$</formula></para></entry><entry thead="no"><para><formula id="28">$R = 0$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Least-squares using a ridge (L1) penalty</para></entry><entry thead="no"><para><formula id="27">$L = (a - y)^2$</formula></para></entry><entry thead="no"><para><formula id="29">$R = \lambda |w|$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Least-squares using a lasso (L2) penalty</para></entry><entry thead="no"><para><formula id="27">$L = (a - y)^2$</formula></para></entry><entry thead="no"><para><formula id="30">$R = \lambda w^2/2$</formula> </para></entry></row>
<row>
<entry thead="no"><para></para></entry></row>
<row>
<entry thead="no"><para>Logistic regression without regularization</para></entry><entry thead="no"><para><formula id="31">$L = \log(1+\exp(-a y))$</formula></para></entry><entry thead="no"><para><formula id="28">$R = 0$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Logistic regression using a L1 penalty</para></entry><entry thead="no"><para><formula id="31">$L = \log(1+\exp(-a y))$</formula></para></entry><entry thead="no"><para><formula id="29">$R = \lambda |w|$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Logistic regression using a L2 penalty</para></entry><entry thead="no"><para><formula id="31">$L = \log(1+\exp(-a y))$</formula></para></entry><entry thead="no"><para><formula id="30">$R = \lambda w^2/2$</formula> </para></entry></row>
<row>
<entry thead="no"><para></para></entry></row>
<row>
<entry thead="no"><para>Support Vector Machine without regularization</para></entry><entry thead="no"><para><formula id="32">$L = \max(0, 1-a y)$</formula></para></entry><entry thead="no"><para><formula id="28">$R = 0$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Support Vector Machine using a L1 penalty</para></entry><entry thead="no"><para><formula id="32">$L = \max(0, 1-a y)$</formula></para></entry><entry thead="no"><para><formula id="29">$R = \lambda |w|$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Support Vector Machine using a L2 penalty</para></entry><entry thead="no"><para><formula id="32">$L = \max(0, 1-a y)$</formula></para></entry><entry thead="no"><para><formula id="30">$R = \lambda w^2/2$</formula> </para></entry></row>
</table>
</para><para><heading level="3">Supported Algorithms</heading>
</para><para><heading level="4">Logistic Regression</heading>
</para><para>Logistic regression is a special case of generalized linear models. These models assume that the data points are generated from a non-linear transformation of a linear function. If we call this non-linear function as <formula id="33">$y = \theta(s)$</formula>, <formula id="34">$s$</formula> will be the output of the linear function <formula id="35">\[s=w^{T} x\]</formula> where <formula id="36">$x_n \in{R^d}$</formula> correspond to input vector with <formula id="37">$d$</formula> dimensions and <formula id="38">$y_n \in{\{-1,+1\}}$</formula> is the associated label for the data vector <formula id="39">$x_n$</formula>. Here if the <formula id="40">$\theta(s)$</formula> is selected as the <formula id="41">\[\theta(s) = s\]</formula> the problem turns into the liner regression where the output has no bounds and for the <formula id="42">\[\theta(s) = sign(s)\]</formula> the model becomes the linear classifier where the outputs are <formula id="43">$-1$</formula> and <formula id="44">$1$</formula>.</para><para>For the logistic regression, the non-linear function is selected as <formula id="45">\[\theta(s) = \frac{e^s}{1+e^s}\]</formula> which we can pretend to outputs as the probability values. Here we can express the above equation in terms of probabilities as <formula id="46">\[P(y=1 | x)= \theta(s) = \frac{1}{1+e^{-w^{T} x}}\]</formula> and <formula id="47">\[P(y=-1 | x)= 1-\theta(s) = \frac{e^{-w^{T} x}}{1+e^{-w^{T} x}}=\frac{1}{1+e^{w^{T} x}}\]</formula></para><para>Note that we derive the second equation using the fact that the sum of two probabilities <formula id="48">$P(y=1 | x)$</formula> and <formula id="49">$P(y=0 | x)$</formula> must be one. We can further simplify the likelihood using the fact that <formula id="50">$\theta(s)=1-\theta(-s)$</formula>, so the probabilities becomes <formula id="51">\[P(y|x) = \frac{1}{1+e^{-yw^{T} x}}\]</formula> Here we try to find such weights <formula id="21">$w$</formula> which maximizes these probabilities for all samples <formula id="52">$n=1,2,\dots,N$</formula>. The problem can be expressed in terms of likelihood function as <formula id="53">\[L(x,y|w) = \Pi_{n=1}^N P(y|x) = \Pi_{n=1}^N \frac{1}{1+e^{-yw^{T} x}}\]</formula></para><para>where we try to find <formula id="21">$w$</formula> which maximizes <formula id="54">$L(x,y|w)$</formula>. Since the likelihood function involves exponentials, using log-likelihood of the data create more easy-to-solve equation. In this case the log-likelihood function becomes <formula id="55">\[L(x,y|w) = \sum_{n=1}^N \log\left (\frac{1}{1+e^{-yw^{T} x}}\right )\]</formula> and maximization of this log-likelihood function is equivalent to minimize <formula id="56">\[E(x,y|w) = \sum_{n=1}^N \log\left ({1+e^{-yw^{T} x}}\right )\]</formula></para><para>Note that the resulting error function has not a closed-form which the solution need be found using an iterative algorithm. Moreover, the penalty for the large weights must also be considered to avoid over fitting to data. In this thesis, following towards to automated caricature}, $L_2$ regularized logistic regression is used Dual Coordinate Descent Methods for Logistic Regression}, so the objective function become</para><para><formula id="57">\[\underset{w}{\arg\min} \sum_{n=1}^N \log\left ({1+e^{-yw^{T} x}}\right ) + \frac{\lambda}{2} \sum_{n=1}^{N} w_n^2\]</formula> where the <formula id="58">$\lambda$</formula> is the trade off between matching the training data and the generalization. To solve the above equation we use the gradient descent method <formula id="59">\[w_k^{(t+1)}=w_k^{(t)}-\epsilon \frac{\partial E}{\partial w}\]</formula> where the gradient is <formula id="60">\[\frac{\partial E}{\partial w} = -\sum_{n=1}^N yx\log\left ({1+e^{-yw^{T} x}}\right ) + \lambda \sum_{n=1}^{N} w_n\]</formula></para><para><heading level="4">Support Vector Machine (SVM)</heading>
</para><para>Support Vector Machine (SVM) is a successful learning algorithm that has many uses in pattern recognition for classification and regression cortes1995support,moghaddam2000gender}. The basic idea behind the SVM is finding a hyper-plane or hyper-planes which separate the two classes with a maximum margin, transforming the problem into a quadratic optimization problem. For the problems that cannot be linearly separated in the input space, SVM maps the input space into a much higher-dimensional feature space, where separation is possible. The power of SVM stems from the principle of structural risk minimization that while increasing the classification success, keeps down the VC dimension as much as possible, so that the generalization of the data is maintained. In other words, classification by mapping the data into high-dimensional space and maximizing margin there is the proof of the founded hyper-plane is the best descriptor for that data rather than over-fitting (see Fig. fig:svm}).</para><para>The basic mechanism of SVM may be stated as follows: Given a labeled set of N samples <formula id="61">$(x_n,y_n)$</formula> where <formula id="36">$x_n \in{R^d}$</formula> correspond to input vectors and <formula id="38">$y_n \in{\{-1,+1\}}$</formula> associated label for current vector parsons2005introduction}. A linear classification problem may be expressed in terms of our notation as follows:</para><para><formula id="62">\[f(\mathbf{x})=\mathbf{w}^{T} \phi(\mathbf{x})+b\]</formula></para><para>where <formula id="63">$\phi(\mathbf{x})$</formula> denotes to the feature-space transformation function that makes the data vector <formula id="64">$\mathbf{x}$</formula> linearly separable in transform domain, <formula id="65">$\mathbf{w}$</formula> is the weights for the data in feature space and <formula id="22">$b$</formula> is the bias term. Assuming that the data is linearly separable in feature space, there is at least one <formula id="65">$\mathbf{w}$</formula> and <formula id="22">$b$</formula> that satisfies <formula id="66">$f(\mathbf{x_n})&gt;0$</formula> for <formula id="67">$y_n=+1$</formula> and <formula id="68">$f(x_n)&lt;0$</formula> for <formula id="69">$y_n=-1$</formula>, note that <formula id="70">$y_nf(x_n)&gt;0$</formula> for all training points.</para><para>There might be of course many hyper-plane created by different <formula id="65">$\mathbf{w}$</formula> and <formula id="22">$b$</formula>, SVM aims to find that gives the smallest generalization error by making the hyper-plane as far as possible to the nearest data points also known as support vectors parsons2005introduction}. The distance of a point <formula id="39">$x_n$</formula> to the hyper-plane which is defined by <formula id="71">$\mathbf{w}^{T} \phi(\mathbf{x})+b=0$</formula> is:</para><para><formula id="72">\[\frac{|f(x_n)|}{\Vert\mathbf{w}\Vert}=\frac{y_n(\mathbf{w}^{T} \phi(x_n)+b)}{\Vert\mathbf{w}\Vert}\]</formula></para><para>Note that <formula id="73">$|f(\mathbf{x})|$</formula> can be replaced by <formula id="74">$y_nf(\mathbf{x})$</formula> under the condition that all the data points are correctly classified.</para><para>In order to maximize the distance between hyper-plane and the nearest data point, assuming that the <formula id="39">$x_n$</formula> is the closest point to the plane,we are looking for the arguments</para><para><formula id="75">\[\underset{{\mathbf{w},b}}{\arg\min} \left\{\frac{1}{\Vert \mathbf{w} \Vert} {\min_{n}\left[y_n(\mathbf{w}^{T} \phi(x_n)+b)\right]}\right\}\]</formula></para><para>Because of the complexity of direct solution of the problem, the equation may be expressed as an equivalent problem. Multiplying the distances by a constant in scale domain does not change the problem, so using this as an advantage, we can find a scaling constant that assures:</para><para><formula id="76">\[y_n(\mathbf{w}^{T} \phi(x_n)+b)=1\]</formula></para><para>Note that this scaling also assures that:</para><para><formula id="77">\[y_n(\mathbf{w}^{T} \phi(x_n)+b)\geq 1\]</formula></para><para>In order to solve maximization problem, the problem is quickly transformed into quadratic minimization problem:</para><para><formula id="78">\[\underset{{\mathbf{w},b}}{\arg\min}\frac{1}{2}\mathbf{w}^{T}\mathbf{w}\]</formula></para><para>Note that we still solve the equivalent problem under the constrain that <formula id="79">$y_n(\mathbf{w}^{T} \phi(x_n)+b)\geq1$</formula>. One can simply notice that this constrained optimization problem may easily be solved using Lagrange multipliers</para><para><formula id="80">\[L(\mathbf{w},b,\mathbf{a})=\frac{1}{\Vert\mathbf{w}\Vert ^2}-\sum_{n=1}^{N}a_n\left\{y_n(\mathbf{w}^{T} \phi(x_n)+b)-1\right\}\]</formula></para><para>Here <formula id="81">$a_n$</formula> is an non-negative multiplier for each constrain, for the minus sign in front of the <formula id="81">$a_n$</formula>, problem can also be considered as an maximization problem with respect to the <formula id="81">$a_n$</formula>. In order to find a solution to this quadratic problem, derivation of the equation with respect to <formula id="65">$\mathbf{w}$</formula> and <formula id="22">$b$</formula> must be set to zero.</para><para><formula id="82">\[\frac{\partial{L}}{\partial\mathbf{w}}=\mathbf{w}-\sum_{n=1}^N{a_ny_n\phi(x_n)}\]</formula></para><para><formula id="83">\[\frac{\partial{L}}{\partial{b}}=-\sum_{n=1}^N{a_ny_n}\]</formula></para><para>Substituting these equations into the original one, we come up with a dual representation problem:</para><para><formula id="84">\[\hat{L}(a)=\sum_{n=1}^N{a_n}-\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N{a_na_my_ny_m\phi(x_n)^{T} \phi(x_m)}\]</formula></para><para>which maximize the margin with respect to the <formula id="81">$a_n$</formula> under the following constrains:</para><para><formula id="85">\[a_n\geq 0, \quad for\; n=1,2,...N\]</formula></para><para><formula id="86">\[\sum_{n=1}^N{a_ny_n}=0\]</formula></para><para>Note that the <formula id="87">$\phi(x)$</formula> could be a transform function that represent inputs in an infinite dimension space. However the solution of the problem only needs the dot product of <formula id="88">$\phi(x_n)^{T} \phi(x_m)$</formula>, so for the solution of the problem, the direct computation of the high dimension feature space is not necessary. Defining a <formula id="89">$K(x_n,x_m)$</formula> kernel function which consist of dot products of the input vector, the problem could be solved in low complexity. Definition of the <formula id="87">$\phi(x)$</formula> function determines the kernel function, some of the kernel function frequently used with SVM given as follows:</para><para><table rows="4" cols="2"><row>
<entry thead="yes"><para>kernel</para></entry><entry thead="yes"><para>formula </para></entry></row>
<row>
<entry thead="no"><para>Linear</para></entry><entry thead="no"><para><formula id="90">$K(x_n,x_m)=x_n^{T} x_m$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Polynomial</para></entry><entry thead="no"><para><formula id="91">$K(x_n,x_m)=(x_n^{T} x_m+c)^d$</formula> </para></entry></row>
<row>
<entry thead="no"><para>Radial Basis</para></entry><entry thead="no"><para><formula id="92">$K(x_n,x_m)=\exp(\frac{\Vert x_n-x_m \Vert_2^2}{\sigma^2}$</formula> </para></entry></row>
</table>
</para><para>The <formula id="93">$a_n\geq 0$</formula>&apos;s coming from the solution shows the support vectors, so the equation of the hyper-plane is defined by:</para><para><formula id="94">\[ \mathbf{w}=\sum_{n\in SV}{a_ny_nx_n} \]</formula> </para>    </detaileddescription>
    <location file="C:/Users/cescript/CLionProjects/imlab/include/header/mlcore.h" line="35" column="1"/>
    <listofallmembers>
    </listofallmembers>
  </compounddef>
</doxygen>
