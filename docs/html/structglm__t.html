<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.5"/>
<title>IMLAB Image Processing and Matrix Library: glm_t Struct Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">IMLAB Image Processing and Matrix Library
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.5 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li><a href="classes.html"><span>Data&#160;Structure&#160;Index</span></a></li>
      <li><a href="functions.html"><span>Data&#160;Fields</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('structglm__t.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Data Structures</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">glm_t Struct Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p><a class="el" href="structglm__t.html" title="glm_t struct keeps the given GLM parameters in a single variable and computes the necessary parameter...">glm_t</a> struct keeps the given GLM parameters in a single variable and computes the necessary parameters at the construction step.  
 <a href="structglm__t.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="mlcore_8h_source.html">mlcore.h</a>&gt;</code></p>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p><a class="el" href="structglm__t.html" title="glm_t struct keeps the given GLM parameters in a single variable and computes the necessary parameter...">glm_t</a> struct keeps the given GLM parameters in a single variable and computes the necessary parameters at the construction step. </p>
<p>Generalized Linear Models are machine learning techniques that approximates the output labels \(y\) as the following linear form. </p>
<p class="formulaDsp">
\[y = w \times x + b\]
</p>
<p> Here \(w\) is the linear coefficient and \(b\) is the bias. The problem of the GLM are to find the best coefficient vector and bias value depending on the given restrictions on these vectors. In the most generalized way the cost function can be defined as the sum of the classification and regularization loss </p>
<p class="formulaDsp">
\[C(w,b) := \frac{1}{n} \sum_i { L( y_i, a_i = w_i \times x_i+b) } + R( w, \lambda )\]
</p>
<p>Using the above cost function one can create different sets of problems/solutions by simply changing the lost function \(L\) and regularization function \(R\). Here a list of supported lost and regularization functions are given.</p>
<table class="doxtable">
<tr>
<th>algorithm</th><th>lost function \((L)\)</th><th>regularization \((R)\) </th></tr>
<tr>
<td>Least-squares without regularization</td><td>\(L = (a - y)^2\)</td><td>\(R = 0\) </td></tr>
<tr>
<td>Least-squares using a ridge (L1) penalty</td><td>\(L = (a - y)^2\)</td><td>\(R = \lambda |w|\) </td></tr>
<tr>
<td>Least-squares using a lasso (L2) penalty</td><td>\(L = (a - y)^2\)</td><td>\(R = \lambda w^2/2\) </td></tr>
<tr>
<td colspan="3"></td></tr>
<tr>
<td>Logistic regression without regularization</td><td>\(L = \log(1+\exp(-a y))\)</td><td>\(R = 0\) </td></tr>
<tr>
<td>Logistic regression using a L1 penalty</td><td>\(L = \log(1+\exp(-a y))\)</td><td>\(R = \lambda |w|\) </td></tr>
<tr>
<td>Logistic regression using a L2 penalty</td><td>\(L = \log(1+\exp(-a y))\)</td><td>\(R = \lambda w^2/2\) </td></tr>
<tr>
<td colspan="3"></td></tr>
<tr>
<td>Support Vector Machine without regularization</td><td>\(L = \max(0, 1-a y)\)</td><td>\(R = 0\) </td></tr>
<tr>
<td>Support Vector Machine using a L1 penalty</td><td>\(L = \max(0, 1-a y)\)</td><td>\(R = \lambda |w|\) </td></tr>
<tr>
<td>Support Vector Machine using a L2 penalty</td><td>\(L = \max(0, 1-a y)\)</td><td>\(R = \lambda w^2/2\) </td></tr>
</table>
<h3>Supported Algorithms</h3>
<h4>Logistic Regression</h4>
<p>Logistic regression is a special case of generalized linear models. These models assume that the data points are generated from a non-linear transformation of a linear function. If we call this non-linear function as \(y = \theta(s)\), \(s\) will be the output of the linear function </p>
<p class="formulaDsp">
\[s=w^{T} x\]
</p>
<p> where \(x_n \in{R^d}\) correspond to input vector with \(d\) dimensions and \(y_n \in{\{-1,+1\}}\) is the associated label for the data vector \(x_n\). Here if the \(\theta(s)\) is selected as the </p>
<p class="formulaDsp">
\[\theta(s) = s\]
</p>
<p> the problem turns into the liner regression where the output has no bounds and for the </p>
<p class="formulaDsp">
\[\theta(s) = sign(s)\]
</p>
<p> the model becomes the linear classifier where the outputs are \(-1\) and \(1\).</p>
<p>For the logistic regression, the non-linear function is selected as </p>
<p class="formulaDsp">
\[\theta(s) = \frac{e^s}{1+e^s}\]
</p>
<p> which we can pretend to outputs as the probability values. Here we can express the above equation in terms of probabilities as </p>
<p class="formulaDsp">
\[P(y=1 | x)= \theta(s) = \frac{1}{1+e^{-w^{T} x}}\]
</p>
<p> and </p>
<p class="formulaDsp">
\[P(y=-1 | x)= 1-\theta(s) = \frac{e^{-w^{T} x}}{1+e^{-w^{T} x}}=\frac{1}{1+e^{w^{T} x}}\]
</p>
<p>Note that we derive the second equation using the fact that the sum of two probabilities \(P(y=1 | x)\) and \(P(y=0 | x)\) must be one. We can further simplify the likelihood using the fact that \(\theta(s)=1-\theta(-s)\), so the probabilities becomes </p>
<p class="formulaDsp">
\[P(y|x) = \frac{1}{1+e^{-yw^{T} x}}\]
</p>
<p> Here we try to find such weights \(w\) which maximizes these probabilities for all samples \(n=1,2,\dots,N\). The problem can be expressed in terms of likelihood function as </p>
<p class="formulaDsp">
\[L(x,y|w) = \Pi_{n=1}^N P(y|x) = \Pi_{n=1}^N \frac{1}{1+e^{-yw^{T} x}}\]
</p>
<p>where we try to find \(w\) which maximizes \(L(x,y|w)\). Since the likelihood function involves exponentials, using log-likelihood of the data create more easy-to-solve equation. In this case the log-likelihood function becomes </p>
<p class="formulaDsp">
\[L(x,y|w) = \sum_{n=1}^N \log\left (\frac{1}{1+e^{-yw^{T} x}}\right )\]
</p>
<p> and maximization of this log-likelihood function is equivalent to minimize </p>
<p class="formulaDsp">
\[E(x,y|w) = \sum_{n=1}^N \log\left ({1+e^{-yw^{T} x}}\right )\]
</p>
<p>Note that the resulting error function has not a closed-form which the solution need be found using an iterative algorithm. Moreover, the penalty for the large weights must also be considered to avoid over fitting to data. In this thesis, following <b>[towards]</b> to automated caricature}, $L_2$ regularized logistic regression is used <b>[Dual]</b> Coordinate Descent Methods for Logistic Regression}, so the objective function become</p>
<p class="formulaDsp">
\[\underset{w}{\arg\min} \sum_{n=1}^N \log\left ({1+e^{-yw^{T} x}}\right ) + \frac{\lambda}{2} \sum_{n=1}^{N} w_n^2\]
</p>
<p> where the \(\lambda\) is the trade off between matching the training data and the generalization. To solve the above equation we use the gradient descent method </p>
<p class="formulaDsp">
\[w_k^{(t+1)}=w_k^{(t)}-\epsilon \frac{\partial E}{\partial w}\]
</p>
<p> where the gradient is </p>
<p class="formulaDsp">
\[\frac{\partial E}{\partial w} = -\sum_{n=1}^N yx\log\left ({1+e^{-yw^{T} x}}\right ) + \lambda \sum_{n=1}^{N} w_n\]
</p>
<h4>Support Vector Machine (SVM)</h4>
<p>Support Vector Machine (SVM) is a successful learning algorithm that has many uses in pattern recognition for classification and regression <b>[cortes1995support]</b>,moghaddam2000gender}. The basic idea behind the SVM is finding a hyper-plane or hyper-planes which separate the two classes with a maximum margin, transforming the problem into a quadratic optimization problem. For the problems that cannot be linearly separated in the input space, SVM maps the input space into a much higher-dimensional feature space, where separation is possible. The power of SVM stems from the principle of structural risk minimization that while increasing the classification success, keeps down the VC dimension as much as possible, so that the generalization of the data is maintained. In other words, classification by mapping the data into high-dimensional space and maximizing margin there is the proof of the founded hyper-plane is the best descriptor for that data rather than over-fitting (see Fig. fig:svm}).</p>
<p>The basic mechanism of SVM may be stated as follows: Given a labeled set of N samples \((x_n,y_n)\) where \(x_n \in{R^d}\) correspond to input vectors and \(y_n \in{\{-1,+1\}}\) associated label for current vector <b>[parsons2005introduction]</b>}. A linear classification problem may be expressed in terms of our notation as follows:</p>
<p class="formulaDsp">
\[f(\mathbf{x})=\mathbf{w}^{T} \phi(\mathbf{x})+b\]
</p>
<p>where \(\phi(\mathbf{x})\) denotes to the feature-space transformation function that makes the data vector \(\mathbf{x}\) linearly separable in transform domain, \(\mathbf{w}\) is the weights for the data in feature space and \(b\) is the bias term. Assuming that the data is linearly separable in feature space, there is at least one \(\mathbf{w}\) and \(b\) that satisfies \(f(\mathbf{x_n})&gt;0\) for \(y_n=+1\) and \(f(x_n)&lt;0\) for \(y_n=-1\), note that \(y_nf(x_n)&gt;0\) for all training points.</p>
<p>There might be of course many hyper-plane created by different \(\mathbf{w}\) and \(b\), SVM aims to find that gives the smallest generalization error by making the hyper-plane as far as possible to the nearest data points also known as support vectors <b>[parsons2005introduction]</b>}. The distance of a point \(x_n\) to the hyper-plane which is defined by \(\mathbf{w}^{T} \phi(\mathbf{x})+b=0\) is:</p>
<p class="formulaDsp">
\[\frac{|f(x_n)|}{\Vert\mathbf{w}\Vert}=\frac{y_n(\mathbf{w}^{T} \phi(x_n)+b)}{\Vert\mathbf{w}\Vert}\]
</p>
<p>Note that \(|f(\mathbf{x})|\) can be replaced by \(y_nf(\mathbf{x})\) under the condition that all the data points are correctly classified.</p>
<p>In order to maximize the distance between hyper-plane and the nearest data point, assuming that the \(x_n\) is the closest point to the plane,we are looking for the arguments</p>
<p class="formulaDsp">
\[\underset{{\mathbf{w},b}}{\arg\min} \left\{\frac{1}{\Vert \mathbf{w} \Vert} {\min_{n}\left[y_n(\mathbf{w}^{T} \phi(x_n)+b)\right]}\right\}\]
</p>
<p>Because of the complexity of direct solution of the problem, the equation may be expressed as an equivalent problem. Multiplying the distances by a constant in scale domain does not change the problem, so using this as an advantage, we can find a scaling constant that assures:</p>
<p class="formulaDsp">
\[y_n(\mathbf{w}^{T} \phi(x_n)+b)=1\]
</p>
<p>Note that this scaling also assures that:</p>
<p class="formulaDsp">
\[y_n(\mathbf{w}^{T} \phi(x_n)+b)\geq 1\]
</p>
<p>In order to solve maximization problem, the problem is quickly transformed into quadratic minimization problem:</p>
<p class="formulaDsp">
\[\underset{{\mathbf{w},b}}{\arg\min}\frac{1}{2}\mathbf{w}^{T}\mathbf{w}\]
</p>
<p>Note that we still solve the equivalent problem under the constrain that \(y_n(\mathbf{w}^{T} \phi(x_n)+b)\geq1\). One can simply notice that this constrained optimization problem may easily be solved using Lagrange multipliers</p>
<p class="formulaDsp">
\[L(\mathbf{w},b,\mathbf{a})=\frac{1}{\Vert\mathbf{w}\Vert ^2}-\sum_{n=1}^{N}a_n\left\{y_n(\mathbf{w}^{T} \phi(x_n)+b)-1\right\}\]
</p>
<p>Here \(a_n\) is an non-negative multiplier for each constrain, for the minus sign in front of the \(a_n\), problem can also be considered as an maximization problem with respect to the \(a_n\). In order to find a solution to this quadratic problem, derivation of the equation with respect to \(\mathbf{w}\) and \(b\) must be set to zero.</p>
<p class="formulaDsp">
\[\frac{\partial{L}}{\partial\mathbf{w}}=\mathbf{w}-\sum_{n=1}^N{a_ny_n\phi(x_n)}\]
</p>
<p class="formulaDsp">
\[\frac{\partial{L}}{\partial{b}}=-\sum_{n=1}^N{a_ny_n}\]
</p>
<p>Substituting these equations into the original one, we come up with a dual representation problem:</p>
<p class="formulaDsp">
\[\hat{L}(a)=\sum_{n=1}^N{a_n}-\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N{a_na_my_ny_m\phi(x_n)^{T} \phi(x_m)}\]
</p>
<p>which maximize the margin with respect to the \(a_n\) under the following constrains:</p>
<p class="formulaDsp">
\[a_n\geq 0, \quad for\; n=1,2,...N\]
</p>
<p class="formulaDsp">
\[\sum_{n=1}^N{a_ny_n}=0\]
</p>
<p>Note that the \(\phi(x)\) could be a transform function that represent inputs in an infinite dimension space. However the solution of the problem only needs the dot product of \(\phi(x_n)^{T} \phi(x_m)\), so for the solution of the problem, the direct computation of the high dimension feature space is not necessary. Defining a \(K(x_n,x_m)\) kernel function which consist of dot products of the input vector, the problem could be solved in low complexity. Definition of the \(\phi(x)\) function determines the kernel function, some of the kernel function frequently used with SVM given as follows:</p>
<table class="doxtable">
<tr>
<th>kernel</th><th>formula </th></tr>
<tr>
<td>Linear</td><td>\(K(x_n,x_m)=x_n^{T} x_m\) </td></tr>
<tr>
<td>Polynomial</td><td>\(K(x_n,x_m)=(x_n^{T} x_m+c)^d\) </td></tr>
<tr>
<td>Radial Basis</td><td>\(K(x_n,x_m)=\exp(\frac{\Vert x_n-x_m \Vert_2^2}{\sigma^2}\) </td></tr>
</table>
<p>The \(a_n\geq 0\)'s coming from the solution shows the support vectors, so the equation of the hyper-plane is defined by:</p>
<p class="formulaDsp">
\[ \mathbf{w}=\sum_{n\in SV}{a_ny_nx_n} \]
</p>
 </div><hr/>The documentation for this struct was generated from the following file:<ul>
<li>E:/imlab_library/include/<a class="el" href="mlcore_8h_source.html">mlcore.h</a></li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="structglm__t.html">glm_t</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.5 </li>
  </ul>
</div>
</body>
</html>
